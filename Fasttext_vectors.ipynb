{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions for assessing word vector similarity are lifted from [these worked examples](https://medium.com/swlh/playing-with-word-vectors-308ab2faa519) without the type checking cruft; there are likely quicker ways!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_len(v):\n",
    "    return math.sqrt(sum([x*x for x in v]))\n",
    "\n",
    "def dot_product(v1, v2):\n",
    "    assert len(v1) == len(v2)\n",
    "    return sum([x*y for (x,y) in zip(v1, v2)])\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"\n",
    "    Returns the cosine of the angle between the two vectors.\n",
    "    Results range from -1 (very different) to 1 (very similar).\n",
    "    \"\"\"\n",
    "    return dot_product(v1, v2) / (vector_len(v1) * vector_len(v2))\n",
    "\n",
    "def sorted_by_similarity(model, base_vector):\n",
    "    \"\"\"Returns words sorted by cosine distance to a given vector, most similar first\"\"\"\n",
    "    words_with_distance = [(cosine_similarity(base_vector, model.get_word_vector(w)), w) for w in model.words]\n",
    "    # We want cosine similarity to be as large as possible (close to 1)\n",
    "    return sorted(words_with_distance, key=lambda t: t[0], reverse=True)\n",
    "\n",
    "def related(model, text):\n",
    "    sorted_words = [\n",
    "        word for (dist, word) in\n",
    "            sorted_by_similarity(model, model[text])\n",
    "            if word.lower() != text.lower()\n",
    "        ]\n",
    "    print(', '.join(sorted_words[:20]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('output/results.csv')\n",
    "with open('phrases.txt', 'w') as text:\n",
    "    text.write('\\n'.join(list(data['phrase'])))\n",
    "    \n",
    "model = fasttext.train_unsupervised('phrases.txt', model='skipgram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"microbial_mats.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a word from the phrase collection, find others that have similar vectors. \n",
    "\n",
    "Almost certainly restating the obvious but you can see how we might combine the topic clusters and the word vectors to dig around a much larger corpus; or compare distances between geochemical terms and different concepts; or compare the differences in distances between phrase collections which are pinned to a geographic area. There is likely to be an area of related-but-not-too-related concepts which could show up interesting things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "related(model, 'microbialaminites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
